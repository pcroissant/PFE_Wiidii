{
 "cells": [
  {
   "source": [
    "# Génération d'un dataset augmenté\n",
    "Le but de ce notebook est d'augmenter le dataset donné dans le but de tester la résistance du modèle à ces différentes augmentation.\n",
    "### Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import random\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "#NLPAUG IMPORTS\n",
    "import nlpaug.augmenter.char as char_aug\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "#SKLEARN IMPORTS\n",
    "import sklearn.model_selection as skms"
   ]
  },
  {
   "source": [
    "### Chargement du dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD DATASET\n",
    "with open(\"dataset.json\", \"r\") as dataset_file:\n",
    "   dataset = json.load(dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Intent: irrelevant, occurence: 3413\nIntent: find-restaurant, occurence: 419\nIntent: find-hotel, occurence: 299\nIntent: provide-showtimes, occurence: 124\nIntent: purchase, occurence: 553\nIntent: find-around-me, occurence: 362\nIntent: find-flight, occurence: 113\nIntent: find-train, occurence: 112\n"
     ]
    }
   ],
   "source": [
    "grouped_dataset = {}\n",
    "for data in dataset:\n",
    "    if data.get('intent') in grouped_dataset.keys():\n",
    "        grouped_dataset[data.get('intent')].append(data.get('sentence'))\n",
    "    else:\n",
    "        grouped_dataset[data.get('intent')] = [data.get('sentence')]\n",
    "\n",
    "for intent in grouped_dataset.keys():\n",
    "    random.shuffle(grouped_dataset.get(intent))\n",
    "    print(f\"Intent: {intent}, occurence: {len(grouped_dataset.get(intent))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset 1 : 1346\nDataset 2 : 1346\nDataset 3 : 1346\nDataset 4 : 1346\n"
     ]
    }
   ],
   "source": [
    "dataset_1 = []\n",
    "dataset_2 = []\n",
    "dataset_3 = []\n",
    "dataset_4 = []\n",
    "\n",
    "for intent in grouped_dataset.keys():\n",
    "    sentences = grouped_dataset.get(intent)\n",
    "    chunck_size = len(sentences)//4\n",
    "    #rest = len(sentences)%4\n",
    "\n",
    "    for i in range(0,chunck_size):\n",
    "        dataset_1.append({\"intent\":intent,\"sentence\":sentences[i]})\n",
    "    for j in range(chunck_size, chunck_size*2):\n",
    "        dataset_2.append({\"intent\":intent,\"sentence\":sentences[j]})\n",
    "    for k in range(chunck_size*2, chunck_size*3):\n",
    "        dataset_3.append({\"intent\":intent,\"sentence\":sentences[k]})\n",
    "    for l in range(chunck_size*3, chunck_size*4):\n",
    "        dataset_4.append({\"intent\":intent,\"sentence\":sentences[l]})\n",
    "\n",
    "print(f\"Dataset 1 : {len(dataset_1)}\")\n",
    "print(f\"Dataset 2 : {len(dataset_2)}\")\n",
    "print(f\"Dataset 3 : {len(dataset_3)}\")\n",
    "print(f\"Dataset 4 : {len(dataset_4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1346/1346 [02:16<00:00,  9.83it/s]\n"
     ]
    }
   ],
   "source": [
    "#KEYBOARD AUGMENTATION\n",
    "dataset_aug_1 = []\n",
    "key_aug_1 = char_aug.KeyboardAug(lang=\"fr\",aug_char_min=1, aug_char_max=1,aug_word_min=1,aug_word_max=1,include_upper_case=True,model_path=\"keyboard/fr-mobile.json\")\n",
    "for i in tqdm.trange(len(dataset_1)):\n",
    "    keyboard_aug = key_aug_1.augment(dataset_1[i].get('sentence'), n=1)\n",
    "    dataset_aug_1.append({\"intent\":dataset_1[i].get(\"intent\"),\"sentence\":dataset_1[i].get(\"sentence\"), \"keyboard_aug\":keyboard_aug})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_letter(sentence):\n",
    "    index = random.randint(0, len(sentence)-1)\n",
    "    while sentence[index] == \" \":\n",
    "        index = random.randint(0, len(sentence)-1)\n",
    "    sentence_aug = f\"{sentence[:index]}{sentence[index]}{sentence[index:]}\"\n",
    "    return sentence_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1346/1346 [00:00<00:00, 177315.03it/s]\n"
     ]
    }
   ],
   "source": [
    "#LETTER AUGMENTATION\n",
    "dataset_aug_2 = []\n",
    "for i in tqdm.trange(len(dataset_2)):\n",
    "    letter_aug = augment_letter(dataset_2[i].get('sentence'))\n",
    "    dataset_aug_2.append({\"intent\":dataset_2[i].get(\"intent\"),\"sentence\":dataset_2[i].get(\"sentence\"), \"letter_aug\":letter_aug})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_letter(sentence):\n",
    "    index = random.randint(0, len(sentence)-1)\n",
    "    while sentence[index] == \" \":\n",
    "        index = random.randint(0, len(sentence)-1)\n",
    "    sentence_aug = f\"{sentence[:index]}{sentence[index+1:]}\"\n",
    "    return sentence_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1346/1346 [00:00<00:00, 335723.90it/s]\n"
     ]
    }
   ],
   "source": [
    "#LETTER DELETE\n",
    "dataset_aug_3 = []\n",
    "for i in tqdm.trange(len(dataset_3)):\n",
    "    letter_del = delete_letter(dataset_3[i].get('sentence'))\n",
    "    dataset_aug_3.append({\"intent\":dataset_3[i].get(\"intent\"),\"sentence\":dataset_3[i].get(\"sentence\"), \"letter_del\":letter_del})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1346/1346 [02:16<00:00,  9.85it/s]\n"
     ]
    }
   ],
   "source": [
    "#WORD AUGMENTATION\n",
    "dataset_aug_4 = []\n",
    "wordswap_aug = naw.RandomWordAug(action=\"swap\")\n",
    "for i in tqdm.trange(len(dataset_4)):\n",
    "    word_aug = wordswap_aug.augment(dataset_4[i].get('sentence'), n=1)\n",
    "    dataset_aug_4.append({\"intent\":dataset_4[i].get(\"intent\"),\"sentence\":dataset_4[i].get(\"sentence\"), \"word_aug\":word_aug})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE AUGMENTED DATASETS\n",
    "with open(\"datasets/dataset_keyaug.json\", \"w\") as dataset_keyaug:\n",
    "    json.dump(dataset_aug_1, dataset_keyaug, ensure_ascii=False)\n",
    "\n",
    "with open(\"datasets/dataset_letteraug.json\", \"w\") as dataset_letteraug:\n",
    "    json.dump(dataset_aug_2, dataset_letteraug, ensure_ascii=False)\n",
    "\n",
    "with open(\"datasets/dataset_letterdel.json\", \"w\") as dataset_letterdel:\n",
    "    json.dump(dataset_aug_3, dataset_letterdel, ensure_ascii=False)\n",
    "\n",
    "with open(\"datasets/dataset_wordaug.json\", \"w\") as dataset_wordaug:\n",
    "    json.dump(dataset_aug_4, dataset_wordaug, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'intent': 'irrelevant', 'sentence': 'je viens de dire à Noël mongol', 'keyboard_aug': 'je viens de dire à joël mongol'}\n"
     ]
    }
   ],
   "source": [
    "#MERGE DATASETS\n",
    "dataset_aug = []\n",
    "dataset_aug.extend(dataset_aug_1)\n",
    "dataset_aug.extend(dataset_aug_2)\n",
    "dataset_aug.extend(dataset_aug_3)\n",
    "dataset_aug.extend(dataset_aug_4)\n",
    "\n",
    "print(dataset_aug[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE AUGMENTED DATASET\n",
    "with open(\"datasets/dataset_aug.json\", \"w\") as dataset_aug_file:\n",
    "    json.dump(dataset_aug, dataset_aug_file, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('venv')",
   "language": "python",
   "name": "python36964bitvenv41d417840fc94b5ba4c45cbb5a2bf6fe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}